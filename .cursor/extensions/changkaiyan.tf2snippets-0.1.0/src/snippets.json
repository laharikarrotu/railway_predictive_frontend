{
	"Import Tensorflow Keras": {
		"prefix": "tfk:import",
		"body": [
			"import tensorflow as tf",
			"from tensorflow import keras"
		],
		"description": "Import Tensorflow-Keras Package"
	},
	"Load MNIST": {
		"prefix": "tfk:dataset:mnist",
		"body": [
			"(${1:x_train}, ${2:y_train}), (${3:x_test}, ${4:y_test}) = keras.datasets.mnist.load_data()"
		],
		"description": "Load Mnist Data Set"
	},
	"Load Cifar10": {
		"prefix": "tfk:dataset:cifar10",
		"body": [
			"(${1:x_train}, ${2:y_train}), (${3:x_test}, ${4:y_test}) = keras.datasets.cifar10.load_data()"
		],
		"description": "Load Cifar10 Data Set"
	},
	"Load Cifar100": {
		"prefix": "tfk:dataset:cifar100",
		"body": [
			"(${1:x_train}, ${2:y_train}), (${3:x_test}, ${4:y_test}) = keras.datasets.cifar100.load_data()"
		],
		"description": "Load Cifar100 Data Set"
	},
	"Load celeb_a": {
		"prefix": "tfk:dataset:celeb_a",
		"body": [
			"# import tensorflow_datasets as tfds",
			"(train_dataset, test_dataset, vali_datset), info = tfds.load('celeb_a',",
			"                                                             split=['train',",
			"                                                                    'test',",
			"                                                                    'validation'],",
			"                                                             with_info=True)"
		],
		"description": "Load Celeb_a Data Set"
	},
	"Load coco": {
		"prefix": "tfk:dataset:coco",
		"body": [
			"# import tensorflow_datasets as tfds",
			"dataset, info = tfds.load('coco',with_info=True)"
		],
		"description": "Load Coco Data Set"
	},
	"Load imagenet2012": {
		"prefix": "tfk:dataset:imagenet2012",
		"body": [
			"# import tensorflow_datasets as tfds",
			"#! Warning: Manually default download to ~/tensorflow_datasets/manual/imagenet2012",
			"(train_dataset, test_dataset), info = tfds.load('imagenet2012',",
			"                          split=[\"train\", \"validation\"],",
			"                          with_info=True)"
		],
		"description": "Load Imagenet2012 Data Set"
	},
	"Load voc": {
		"prefix": "tfk:dataset:voc2007",
		"body": [
			"# import tensorflow_datasets as tfds",
			"dataset, info = tfds.load('voc/2007',with_info=True)"
		],
		"description": "Load Voc2007 Data Set"
	},
	"Load iris": {
		"prefix": "tfk:dataset:iris",
		"body": [
			"# import tensorflow_datasets as tfds",
			"dataset, info = tfds.load('iris',with_info=True)"
		],
		"description": "Load Iris Data Set"
	},
	"Load cycle_gan": {
		"prefix": "tfk:dataset:cycle_gan",
		"body": [
			"# import tensorflow_datasets as tfds",
			"dataset, info = tfds.load('cycle_gan',with_info=True)"
		],
		"description": "Load Cycle_gan Data Set"
	},
	"Load scientific_papers": {
		"prefix": "tfk:dataset:scientific_papers",
		"body": [
			"# import tensorflow_datasets as tfds",
			"dataset, info = tfds.load('scientific_papers',with_info=True)"
		],
		"description": "Load Scientific_papers Data Set"
	},
	"Load scicite": {
		"prefix": "tfk:dataset:scicite",
		"body": [
			"# import tensorflow_datasets as tfds",
			"dataset, info = tfds.load('scicite',with_info=True)"
		],
		"description": "Load scicite Data Set"
	},
	"Load flores": {
		"prefix": "tfk:dataset:flores",
		"body": [
			"# import tensorflow_datasets as tfds",
			"dataset, info = tfds.load('flores',with_info=True)"
		],
		"description": "Load Flores Data Set"
	},
	"Load moving_mnist": {
		"prefix": "tfk:dataset:moving_mnist",
		"body": [
			"# import tensorflow_datasets as tfds",
			"dataset, info = tfds.load('moving_mnist',with_info=True)"
		],
		"description": "Load moving_mnist Data Set"
	},
	"Load Fashion MNIST": {
		"prefix": "tfk:dataset:fmnist",
		"body": [
			"(${1:x_train}, ${2:y_train}), (${3:x_test}, ${4:y_test}) = keras.datasets.fashion_mnist.load_data()"
		],
		"description": "Load Fashion Mnist Data Set"
	},
	"Model Block": {
		"prefix": "tfk:ctrl:model",
		"body": [
			"class ${1:MyModel}(tf.keras.Model):",
			"",
			"  def __init__(self):",
			"    super(${1:MyModel}, self).__init__()",
			"    self.dense1 = tf.keras.layers.${2:[Dense Like Layer]}",
			"",
			"  def call(self, inputs):",
			"    x = self.dense1(inputs)",
			"    return x"
		],
		"description": "Create a Model Class"
	},
	"Fit Op": {
		"prefix": "tfk:ctrl:fit",
		"body": [
			"$1.fit(x=$2, y=$3,",
			"          batch_size=None, epochs=$4,",
			"          verbose=1, validation_data=None,",
			"          steps_per_epoch=None, validation_steps=None,",
			"          validation_batch_size=None, validation_freq=1)"
		],
		"description": "Fit Operator in Keras"
	},
	"Evaluate Op": {
		"prefix": "tfk:ctrl:evaluate",
		"body": [
			"$1.evaluate($2, $3, batch_size=None, verbose=1,",
			"               sample_weight=None, steps=None, callbacks=None, max_queue_size=10)"
		],
		"description": "Evaluate Operator in Keras"
	},
	"Compile Op": {
		"prefix": "tfk:ctrl:compile",
		"body": [
			"$1.compile(optimizer='${2:adam}', loss=None, metrics=['accuracy'], loss_weights=None,",
			"              sample_weight_mode=None, weighted_metrics=None)"
		],
		"description": "Compile Operator in Keras"
	},
	"Callback Class": {
		"prefix": "tfk:ctrl:callback",
		"body": [
			"class ${1:FitCallback}(tf.keras.callbacks.Callback):",
			"    def on_epoch_${2:end}(self, epoch, logs=None):",
			"        pass"
		],
		"description": "Callback Class in Fit"
	},
	"Save Model Op": {
		"prefix": "tfk:ctrl:saveModel",
		"body": [
			"tf.keras.models.save_model(",
			"    ${1:model}, ${2:filePathString}, overwrite=True,",
			"    include_optimizer=True, save_format=None,",
			"    signatures=None, options=None)"
		],
		"description": "Save your Model"
	},
	"Load Model Op": {
		"prefix": "tfk:ctrl:loadModel",
		"body": [
			"tf.keras.models.load_model(",
			"    ${1:filePathString},",
			"    custom_objects=None, compile=True)"
		],
		"description": "Load your Model"
	},
	"Simple Code Frame of Mnist": {
		"prefix": "tfk:code:mnist:simple",
		"body": [
			"from tensorflow import keras",
			"",
			"mnist = keras.datasets.mnist",
			"",
			"(x_train, y_train), (x_test, y_test) = mnist.load_data()",
			"x_train, x_test = x_train / 255.0, x_test / 255.0",
			"",
			"model = keras.models.Sequential([",
			"    keras.layers.Flatten(input_shape=(28, 28)),",
			"    keras.layers.Dense(${1:128}, activation='${2:relu}'),",
			"    keras.layers.Dropout(${3: 0.2}),",
			"    keras.layers.Dense(${4:10}, activation='${5:softmax}')",
			"])",
			"",
			"model.compile(optimizer='${6:adam}',",
			"              loss='${7:sparse_categorical_crossentropy}',",
			"              metrics=['accuracy'])",
			"",
			"model.fit(x_train, y_train, epochs=${8:5})",
			"",
			"model.evaluate(x_test,  y_test, verbose=2)"
		],
		"description": "Simple Model Framework of Mnist"
	},
	"Code Frame of Mnist": {
		"prefix": "tfk:code:mnist:full",
		"body": [
			"import tensorflow as tf",
			"from tensorflow import keras",
			"from tensorflow.keras.layers import Dense, Flatten, Conv2D",
			"from tensorflow.keras import Model",
			"",
			"mnist = keras.datasets.mnist",
			"",
			"(x_train, y_train), (x_test, y_test) = mnist.load_data()",
			"x_train, x_test = x_train / 255.0, x_test / 255.0",
			"",
			"x_train = x_train[..., tf.newaxis]",
			"x_test = x_test[..., tf.newaxis]",
			"",
			"train_ds = tf.data.Dataset.from_tensor_slices(",
			"    (x_train, y_train)).shuffle(${1:10000}).batch(${2:32})",
			"test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(${2:32})",
			"",
			"class MyModel(Model):",
			"    def __init__(self):",
			"        super(MyModel, self).__init__()",
			"        self.conv1 = Conv2D(32, 3, activation='relu')",
			"        self.flatten = Flatten()",
			"        self.d1 = Dense(128, activation='relu')",
			"        self.d2 = Dense(10, activation='softmax')",
			"",
			"    def call(self, x):",
			"        x = self.conv1(x)",
			"        x = self.flatten(x)",
			"        x = self.d1(x)",
			"        return self.d2(x)",
			"",
			"",
			"model = MyModel()",
			"",
			"loss_object = tf.keras.losses.SparseCategoricalCrossentropy()",
			"",
			"optimizer = tf.keras.optimizers.Adam()",
			"",
			"train_loss = tf.keras.metrics.Mean(name='train_loss')",
			"train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(",
			"    name='train_accuracy')",
			"",
			"test_loss = tf.keras.metrics.Mean(name='test_loss')",
			"test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(",
			"    name='test_accuracy')",
			"",
			"",
			"@tf.function",
			"def train_step(images, labels):",
			"    with tf.GradientTape() as tape:",
			"        predictions = model(images)",
			"        loss = loss_object(labels, predictions)",
			"    gradients = tape.gradient(loss, model.trainable_variables)",
			"    optimizer.apply_gradients(zip(gradients, model.trainable_variables))",
			"",
			"    train_loss(loss)",
			"    train_accuracy(labels, predictions)",
			"",
			"",
			"@tf.function",
			"def test_step(images, labels):",
			"    predictions = model(images)",
			"    t_loss = loss_object(labels, predictions)",
			"",
			"    test_loss(t_loss)",
			"    test_accuracy(labels, predictions)",
			"",
			"",
			"EPOCHS = 5",
			"",
			"for epoch in range(EPOCHS):",
			"    train_loss.reset_states()",
			"    train_accuracy.reset_states()",
			"    test_loss.reset_states()",
			"    test_accuracy.reset_states()",
			"",
			"    for images, labels in train_ds:",
			"        train_step(images, labels)",
			"",
			"    for test_images, test_labels in test_ds:",
			"        test_step(test_images, test_labels)",
			"",
			"    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'",
			"    print(template.format(epoch+1,",
			"                          train_loss.result(),",
			"                          train_accuracy.result()*100,",
			"                          test_loss.result(),",
			"                          test_accuracy.result()*100))"
		],
		"description": "Model Framework of Mnist"
	},
	"Model Framework of Oxford": {
		"prefix": "tfk:code:oxford",
		"body": [
			"# run command: pip install -q git+https://github.com/tensorflow/examples.git",
			"# run command: pip install tensorflow_datasets",
			"import matplotlib.pyplot as plt",
			"from IPython.display import clear_output",
			"import tensorflow as tf",
			"",
			"from tensorflow_examples.models.pix2pix import pix2pix",
			"",
			"import tensorflow_datasets as tfds",
			"tfds.disable_progress_bar()",
			"",
			"dataset, info = tfds.load('oxford_iiit_pet:3.1.0', with_info=True)",
			"",
			"",
			"def normalize(input_image, input_mask):",
			"    input_image = tf.cast(input_image, tf.float32)/255.0",
			"    input_mask -= 1",
			"    return input_image, input_mask",
			"",
			"",
			"@tf.function",
			"def load_image_train(datapoint):",
			"    input_image = tf.image.resize(datapoint['image'], (128, 128))",
			"    input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))",
			"",
			"    if tf.random.uniform(()) > 0.5:",
			"        input_image = tf.image.flip_left_right(input_image)",
			"        input_mask = tf.image.flip_left_right(input_mask)",
			"",
			"    input_image, input_mask = normalize(input_image, input_mask)",
			"",
			"    return input_image, input_mask",
			"",
			"",
			"def load_image_test(datapoint):",
			"    input_image = tf.image.resize(datapoint['image'], (128, 128))",
			"    input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))",
			"",
			"    input_image, input_mask = normalize(input_image, input_mask)",
			"",
			"    return input_image, input_mask",
			"",
			"",
			"TRAIN_LENGTH = info.splits['train'].num_examples",
			"BATCH_SIZE = 64",
			"BUFFER_SIZE = 1000",
			"STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE",
			"",
			"train = dataset['train'].map(",
			"    load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)",
			"test = dataset['test'].map(load_image_test)",
			"",
			"train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()",
			"train_dataset = train_dataset.prefetch(",
			"    buffer_size=tf.data.experimental.AUTOTUNE)",
			"test_dataset = test.batch(BATCH_SIZE)",
			"",
			"",
			"def display(display_list):",
			"    plt.figure(figsize=(15, 15))",
			"",
			"    title = ['Input Image', 'True Mask', 'Predicted Mask']",
			"",
			"    for i in range(len(display_list)):",
			"        plt.subplot(1, len(display_list), i+1)",
			"        plt.title(title[i])",
			"        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))",
			"        plt.axis('off')",
			"    plt.show()",
			"",
			"",
			"for image, mask in train.take(1):",
			"    sample_image, sample_mask = image, mask",
			"display([sample_image, sample_mask])",
			"",
			"OUTPUT_CHANNELS = 3",
			"",
			"base_model = tf.keras.applications.MobileNetV2(",
			"    input_shape=[128, 128, 3], include_top=False)",
			"",
			"layer_names = [",
			"    'block_1_expand_relu',   # 64x64",
			"    'block_3_expand_relu',   # 32x32",
			"    'block_6_expand_relu',   # 16x16",
			"    'block_13_expand_relu',  # 8x8",
			"    'block_16_project',      # 4x4",
			"]",
			"layers = [base_model.get_layer(name).output for name in layer_names]",
			"",
			"down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)",
			"",
			"down_stack.trainable = False",
			"",
			"up_stack = [",
			"    pix2pix.upsample(512, 3),  # 4x4 -> 8x8",
			"    pix2pix.upsample(256, 3),  # 8x8 -> 16x16",
			"    pix2pix.upsample(128, 3),  # 16x16 -> 32x32",
			"    pix2pix.upsample(64, 3),   # 32x32 -> 64x64",
			"]",
			"",
			"",
			"def unet_model(output_channels):",
			"",
			"    last = tf.keras.layers.Conv2DTranspose(",
			"        output_channels, 3, strides=2,",
			"        padding='same', activation='softmax')  # 64x64 -> 128x128",
			"",
			"    inputs = tf.keras.layers.Input(shape=[128, 128, 3])",
			"    x = inputs",
			"",
			"    skips = down_stack(x)",
			"    x = skips[-1]",
			"    skips = reversed(skips[:-1])",
			"",
			"    for up, skip in zip(up_stack, skips):",
			"        x = up(x)",
			"        concat = tf.keras.layers.Concatenate()",
			"        x = concat([x, skip])",
			"",
			"    x = last(x)",
			"",
			"    return tf.keras.Model(inputs=inputs, outputs=x)",
			"",
			"model = unet_model(OUTPUT_CHANNELS)",
			"model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',",
			"              metrics=['accuracy'])",
			"",
			"def create_mask(pred_mask):",
			"  pred_mask = tf.argmax(pred_mask, axis=-1)",
			"  pred_mask = pred_mask[..., tf.newaxis]",
			"  return pred_mask[0]",
			"",
			"def show_predictions(dataset=None, num=1):",
			"  if dataset:",
			"    for image, mask in dataset.take(num):",
			"      pred_mask = model.predict(image)",
			"      display([image[0], mask[0], create_mask(pred_mask)])",
			"  else:",
			"    display([sample_image, sample_mask,",
			"             create_mask(model.predict(sample_image[tf.newaxis, ...]))])",
			"",
			"show_predictions()",
			"",
			"class DisplayCallback(tf.keras.callbacks.Callback):",
			"  def on_epoch_end(self, epoch, logs=None):",
			"    clear_output(wait=True)",
			"    show_predictions()",
			"    print ('\nSample Prediction after epoch {}\n'.format(epoch+1))",
			"",
			"EPOCHS = 20",
			"VAL_SUBSPLITS = 5",
			"VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS",
			"",
			"model_history = model.fit(train_dataset, epochs=EPOCHS,",
			"                          steps_per_epoch=STEPS_PER_EPOCH,",
			"                          validation_steps=VALIDATION_STEPS,",
			"                          validation_data=test_dataset,",
			"                          callbacks=[DisplayCallback()])",
			"",
			"loss = model_history.history['loss']",
			"val_loss = model_history.history['val_loss']",
			"",
			"epochs = range(EPOCHS)",
			"",
			"plt.figure()",
			"plt.plot(epochs, loss, 'r', label='Training loss')",
			"plt.plot(epochs, val_loss, 'bo', label='Validation loss')",
			"plt.title('Training and Validation Loss')",
			"plt.xlabel('Epoch')",
			"plt.ylabel('Loss Value')",
			"plt.ylim([0, 1])",
			"plt.legend()",
			"plt.show()",
			"",
			"show_predictions(test_dataset, 3)"
		],
		"description": "Model Framework of Oxford"
	},
	"Translate": {
		"prefix": "tfk:code:translate",
		"body": [
			"import tensorflow as tf",
			"",
			"import matplotlib.pyplot as plt",
			"import matplotlib.ticker as ticker",
			"from sklearn.model_selection import train_test_split",
			"",
			"import unicodedata",
			"import re",
			"import numpy as np",
			"import os",
			"import io",
			"import time",
			"",
			"path_to_zip = tf.keras.utils.get_file(",
			"    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',",
			"    extract=True)",
			"",
			"path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"",
			"",
			"# Converts the unicode file to ascii",
			"",
			"",
			"def unicode_to_ascii(s):",
			"    return ''.join(c for c in unicodedata.normalize('NFD', s)",
			"                   if unicodedata.category(c) != 'Mn')",
			"",
			"",
			"def preprocess_sentence(w):",
			"    w = unicode_to_ascii(w.lower().strip())",
			"",
			"    # creating a space between a word and the punctuation following it",
			"    # eg: \"he is a boy.\" => \"he is a boy .\"",
			"    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation",
			"    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)",
			"    w = re.sub(r'[\" \"]+', \" \", w)",
			"",
			"    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")",
			"    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)",
			"",
			"    w = w.rstrip().strip()",
			"",
			"    # adding a start and an end token to the sentence",
			"    # so that the model know when to start and stop predicting.",
			"    w = '<start> ' + w + ' <end>'",
			"    return w",
			"",
			"",
			"en_sentence = u\"May I borrow this book?\"",
			"sp_sentence = u\"¿Puedo tomar prestado este libro?\"",
			"print(preprocess_sentence(en_sentence))",
			"print(preprocess_sentence(sp_sentence).encode('utf-8'))",
			"",
			"# 1. Remove the accents",
			"# 2. Clean the sentences",
			"# 3. Return word pairs in the format: [ENGLISH, SPANISH]",
			"",
			"",
			"def create_dataset(path, num_examples):",
			"    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')",
			"",
			"    word_pairs = [[preprocess_sentence(w) for w in l.split(",
			"        '\\t')] for l in lines[:num_examples]]",
			"",
			"    return zip(*word_pairs)",
			"",
			"",
			"en, sp = create_dataset(path_to_file, None)",
			"print(en[-1])",
			"print(sp[-1])",
			"",
			"",
			"def max_length(tensor):",
			"    return max(len(t) for t in tensor)",
			"",
			"",
			"def tokenize(lang):",
			"    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(",
			"        filters='')",
			"    lang_tokenizer.fit_on_texts(lang)",
			"",
			"    tensor = lang_tokenizer.texts_to_sequences(lang)",
			"",
			"    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,",
			"                                                           padding='post')",
			"",
			"    return tensor, lang_tokenizer",
			"",
			"",
			"def load_dataset(path, num_examples=None):",
			"    # creating cleaned input, output pairs",
			"    targ_lang, inp_lang = create_dataset(path, num_examples)",
			"",
			"    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)",
			"    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)",
			"",
			"    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer",
			"",
			"",
			"# Try experimenting with the size of that dataset",
			"num_examples = 30000",
			"input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(",
			"    path_to_file, num_examples)",
			"",
			"# Calculate max_length of the target tensors",
			"max_length_targ, max_length_inp = max_length(",
			"    target_tensor), max_length(input_tensor)",
			"",
			"# Creating training and validation sets using an 80-20 split",
			"input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(",
			"    input_tensor, target_tensor, test_size=0.2)",
			"",
			"",
			"BUFFER_SIZE = len(input_tensor_train)",
			"BATCH_SIZE = 64",
			"steps_per_epoch = len(input_tensor_train)//BATCH_SIZE",
			"embedding_dim = 256",
			"units = 1024",
			"vocab_inp_size = len(inp_lang.word_index)+1",
			"vocab_tar_size = len(targ_lang.word_index)+1",
			"",
			"dataset = tf.data.Dataset.from_tensor_slices(",
			"    (input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)",
			"dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)",
			"",
			"example_input_batch, example_target_batch = next(iter(dataset))",
			"example_input_batch.shape, example_target_batch.shape",
			"",
			"",
			"class Encoder(tf.keras.Model):",
			"    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):",
			"        super(Encoder, self).__init__()",
			"        self.batch_sz = batch_sz",
			"        self.enc_units = enc_units",
			"        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)",
			"        self.gru = tf.keras.layers.GRU(self.enc_units,",
			"                                       return_sequences=True,",
			"                                       return_state=True,",
			"                                       recurrent_initializer='glorot_uniform')",
			"",
			"    def call(self, x, hidden):",
			"        x = self.embedding(x)",
			"        output, state = self.gru(x, initial_state=hidden)",
			"        return output, state",
			"",
			"    def initialize_hidden_state(self):",
			"        return tf.zeros((self.batch_sz, self.enc_units))",
			"",
			"",
			"encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)",
			"",
			"# sample input",
			"sample_hidden = encoder.initialize_hidden_state()",
			"sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)",
			"print('Encoder output shape: (batch size, sequence length, units) {}'.format(",
			"    sample_output.shape))",
			"print('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))",
			"",
			"",
			"class BahdanauAttention(tf.keras.layers.Layer):",
			"    def __init__(self, units):",
			"        super(BahdanauAttention, self).__init__()",
			"        self.W1 = tf.keras.layers.Dense(units)",
			"        self.W2 = tf.keras.layers.Dense(units)",
			"        self.V = tf.keras.layers.Dense(1)",
			"",
			"    def call(self, query, values):",
			"        # query hidden state shape == (batch_size, hidden size)",
			"        # query_with_time_axis shape == (batch_size, 1, hidden size)",
			"        # values shape == (batch_size, max_len, hidden size)",
			"        # we are doing this to broadcast addition along the time axis to calculate the score",
			"        query_with_time_axis = tf.expand_dims(query, 1)",
			"",
			"        # score shape == (batch_size, max_length, 1)",
			"        # we get 1 at the last axis because we are applying score to self.V",
			"        # the shape of the tensor before applying self.V is (batch_size, max_length, units)",
			"        score = self.V(tf.nn.tanh(",
			"            self.W1(query_with_time_axis) + self.W2(values)))",
			"",
			"        # attention_weights shape == (batch_size, max_length, 1)",
			"        attention_weights = tf.nn.softmax(score, axis=1)",
			"",
			"        # context_vector shape after sum == (batch_size, hidden_size)",
			"        context_vector = attention_weights * values",
			"        context_vector = tf.reduce_sum(context_vector, axis=1)",
			"",
			"        return context_vector, attention_weights",
			"",
			"",
			"attention_layer = BahdanauAttention(10)",
			"attention_result, attention_weights = attention_layer(",
			"    sample_hidden, sample_output)",
			"",
			"print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))",
			"print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(",
			"    attention_weights.shape))",
			"",
			"",
			"class Decoder(tf.keras.Model):",
			"    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):",
			"        super(Decoder, self).__init__()",
			"        self.batch_sz = batch_sz",
			"        self.dec_units = dec_units",
			"        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)",
			"        self.gru = tf.keras.layers.GRU(self.dec_units,",
			"                                       return_sequences=True,",
			"                                       return_state=True,",
			"                                       recurrent_initializer='glorot_uniform')",
			"        self.fc = tf.keras.layers.Dense(vocab_size)",
			"",
			"        # used for attention",
			"        self.attention = BahdanauAttention(self.dec_units)",
			"",
			"    def call(self, x, hidden, enc_output):",
			"        # enc_output shape == (batch_size, max_length, hidden_size)",
			"        context_vector, attention_weights = self.attention(hidden, enc_output)",
			"",
			"        # x shape after passing through embedding == (batch_size, 1, embedding_dim)",
			"        x = self.embedding(x)",
			"",
			"        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)",
			"        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)",
			"",
			"        # passing the concatenated vector to the GRU",
			"        output, state = self.gru(x)",
			"",
			"        # output shape == (batch_size * 1, hidden_size)",
			"        output = tf.reshape(output, (-1, output.shape[2]))",
			"",
			"        # output shape == (batch_size, vocab)",
			"        x = self.fc(output)",
			"",
			"        return x, state, attention_weights",
			"",
			"",
			"decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)",
			"",
			"sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),",
			"                                      sample_hidden, sample_output)",
			"",
			"print('Decoder output shape: (batch_size, vocab size) {}'.format(",
			"    sample_decoder_output.shape))",
			"",
			"optimizer = tf.keras.optimizers.Adam()",
			"loss_object = tf.keras.losses.SparseCategoricalCrossentropy(",
			"    from_logits=True, reduction='none')",
			"",
			"",
			"def loss_function(real, pred):",
			"    mask = tf.math.logical_not(tf.math.equal(real, 0))",
			"    loss_ = loss_object(real, pred)",
			"",
			"    mask = tf.cast(mask, dtype=loss_.dtype)",
			"    loss_ *= mask",
			"",
			"    return tf.reduce_mean(loss_)",
			"",
			"",
			"checkpoint_dir = './training_checkpoints'",
			"checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")",
			"checkpoint = tf.train.Checkpoint(optimizer=optimizer,",
			"                                 encoder=encoder,",
			"                                 decoder=decoder)",
			"",
			"",
			"@tf.function",
			"def train_step(inp, targ, enc_hidden):",
			"    loss = 0",
			"",
			"    with tf.GradientTape() as tape:",
			"        enc_output, enc_hidden = encoder(inp, enc_hidden)",
			"",
			"        dec_hidden = enc_hidden",
			"",
			"        dec_input = tf.expand_dims(",
			"            [targ_lang.word_index['<start>']] * BATCH_SIZE, 1)",
			"",
			"        # Teacher forcing - feeding the target as the next input",
			"        for t in range(1, targ.shape[1]):",
			"            # passing enc_output to the decoder",
			"            predictions, dec_hidden, _ = decoder(",
			"                dec_input, dec_hidden, enc_output)",
			"",
			"            loss += loss_function(targ[:, t], predictions)",
			"",
			"            # using teacher forcing",
			"            dec_input = tf.expand_dims(targ[:, t], 1)",
			"",
			"    batch_loss = (loss / int(targ.shape[1]))",
			"",
			"    variables = encoder.trainable_variables + decoder.trainable_variables",
			"",
			"    gradients = tape.gradient(loss, variables)",
			"",
			"    optimizer.apply_gradients(zip(gradients, variables))",
			"",
			"    return batch_loss",
			"",
			"",
			"EPOCHS = 10",
			"",
			"for epoch in range(EPOCHS):",
			"    start = time.time()",
			"",
			"    enc_hidden = encoder.initialize_hidden_state()",
			"    total_loss = 0",
			"",
			"    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):",
			"        batch_loss = train_step(inp, targ, enc_hidden)",
			"        total_loss += batch_loss",
			"",
			"        if batch % 100 == 0:",
			"            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,",
			"                                                         batch,",
			"                                                         batch_loss.numpy()))",
			"    # saving (checkpoint) the model every 2 epochs",
			"    if (epoch + 1) % 2 == 0:",
			"        checkpoint.save(file_prefix=checkpoint_prefix)",
			"",
			"    print('Epoch {} Loss {:.4f}'.format(epoch + 1,",
			"                                        total_loss / steps_per_epoch))",
			"    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))",
			"",
			"",
			"def evaluate(sentence):",
			"    attention_plot = np.zeros((max_length_targ, max_length_inp))",
			"",
			"    sentence = preprocess_sentence(sentence)",
			"",
			"    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]",
			"    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],",
			"                                                           maxlen=max_length_inp,",
			"                                                           padding='post')",
			"    inputs = tf.convert_to_tensor(inputs)",
			"",
			"    result = ''",
			"",
			"    hidden = [tf.zeros((1, units))]",
			"    enc_out, enc_hidden = encoder(inputs, hidden)",
			"",
			"    dec_hidden = enc_hidden",
			"    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)",
			"",
			"    for t in range(max_length_targ):",
			"        predictions, dec_hidden, attention_weights = decoder(dec_input,",
			"                                                             dec_hidden,",
			"                                                             enc_out)",
			"",
			"        # storing the attention weights to plot later on",
			"        attention_weights = tf.reshape(attention_weights, (-1, ))",
			"        attention_plot[t] = attention_weights.numpy()",
			"",
			"        predicted_id = tf.argmax(predictions[0]).numpy()",
			"",
			"        result += targ_lang.index_word[predicted_id] + ' '",
			"",
			"        if targ_lang.index_word[predicted_id] == '<end>':",
			"            return result, sentence, attention_plot",
			"",
			"        # the predicted ID is fed back into the model",
			"        dec_input = tf.expand_dims([predicted_id], 0)",
			"",
			"    return result, sentence, attention_plot",
			"",
			"# function for plotting the attention weights",
			"",
			"",
			"def plot_attention(attention, sentence, predicted_sentence):",
			"    fig = plt.figure(figsize=(10, 10))",
			"    ax = fig.add_subplot(1, 1, 1)",
			"    ax.matshow(attention, cmap='viridis')",
			"",
			"    fontdict = {'fontsize': 14}",
			"",
			"    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)",
			"    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)",
			"",
			"    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))",
			"    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))",
			"",
			"    plt.show()",
			"",
			"",
			"def translate(sentence):",
			"    result, sentence, attention_plot = evaluate(sentence)",
			"",
			"    print('Input: %s' % (sentence))",
			"    print('Predicted translation: {}'.format(result))",
			"",
			"    attention_plot = attention_plot[:len(",
			"        result.split(' ')), :len(sentence.split(' '))]",
			"    plot_attention(attention_plot, sentence.split(' '), result.split(' '))",
			"",
			"    # restoring the latest checkpoint in checkpoint_dir",
			"checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))",
			"translate(u'hace mucho frio aqui.')"
		],
		"description": "Translate Word by RNN"
	},
	"Word Embedding": {
		"prefix": "tfk:code:word_embeddings",
		"body": [
			"# Run command: pip install -q tf-nightly",
			"",
			"import io",
			"import matplotlib.pyplot as plt",
			"import tensorflow as tf",
			"",
			"from tensorflow import keras",
			"from tensorflow.keras import layers",
			"",
			"import tensorflow_datasets as tfds",
			"tfds.disable_progress_bar()",
			"",
			"(train_data, test_data), info = tfds.load(",
			"    'imdb_reviews/subwords8k',",
			"    split=(tfds.Split.TRAIN, tfds.Split.TEST),",
			"    with_info=True, as_supervised=True)",
			"encoder = info.features['text'].encoder",
			"",
			"",
			"train_batches = train_data.shuffle(1000).padded_batch(10)",
			"test_batches = test_data.shuffle(1000).padded_batch(10)",
			"",
			"train_batch, train_labels = next(iter(train_batches))",
			"",
			"embedding_dim = 16",
			"",
			"model = keras.Sequential([",
			"    layers.Embedding(encoder.vocab_size, embedding_dim),",
			"    layers.GlobalAveragePooling1D(),",
			"    layers.Dense(16, activation='relu'),",
			"    layers.Dense(1)",
			"])",
			"",
			"model.summary()",
			"",
			"model.compile(optimizer='adam',",
			"              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),",
			"              metrics=['accuracy'])",
			"",
			"history = model.fit(",
			"    train_batches,",
			"    epochs=10,",
			"    validation_data=test_batches, validation_steps=20)",
			"",
			"",
			"history_dict = history.history",
			"",
			"acc = history_dict['accuracy']",
			"val_acc = history_dict['val_accuracy']",
			"loss = history_dict['loss']",
			"val_loss = history_dict['val_loss']",
			"",
			"epochs = range(1, len(acc) + 1)",
			"",
			"plt.figure(figsize=(12, 9))",
			"plt.plot(epochs, loss, 'bo', label='Training loss')",
			"plt.plot(epochs, val_loss, 'b', label='Validation loss')",
			"plt.title('Training and validation loss')",
			"plt.xlabel('Epochs')",
			"plt.ylabel('Loss')",
			"plt.legend()",
			"plt.show()",
			"",
			"plt.figure(figsize=(12, 9))",
			"plt.plot(epochs, acc, 'bo', label='Training acc')",
			"plt.plot(epochs, val_acc, 'b', label='Validation acc')",
			"plt.title('Training and validation accuracy')",
			"plt.xlabel('Epochs')",
			"plt.ylabel('Accuracy')",
			"plt.legend(loc='lower right')",
			"plt.ylim((0.5, 1))",
			"plt.show()",
			"",
			"e = model.layers[0]",
			"weights = e.get_weights()[0]",
			"print(weights.shape)  # shape: (vocab_size, embedding_dim)",
			"",
			"",
			"encoder = info.features['text'].encoder",
			"",
			"out_v = io.open('vecs.tsv', 'w', encoding='utf-8')",
			"out_m = io.open('meta.tsv', 'w', encoding='utf-8')",
			"",
			"for num, word in enumerate(encoder.subwords):",
			"    vec = weights[num+1]  # skip 0, it's padding.",
			"    out_m.write(word + \"\\n\")",
			"    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")",
			"out_v.close()",
			"out_m.close()",
			"",
			"try:",
			"    from google.colab import files",
			"except ImportError:",
			"    pass",
			"else:",
			"    files.download('vecs.tsv')",
			"    files.download('meta.tsv')"
		],
		"description": "Word Embedding"
	}
}